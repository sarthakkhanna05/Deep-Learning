{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw9.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45BMVQQcNQ6E"
      },
      "source": [
        "# Hw9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9cDGPI_kjLW"
      },
      "source": [
        "We have to use Deep RL in this HW because if we use normal RL it will not converge to global minima. <br>\n",
        "Because of two major reasons: <br>\n",
        "1. correlation between samples\n",
        "2. Non-stationary targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo5aOqYaVU8C"
      },
      "source": [
        "##### Because of the non-stationary targets problem, I didn't use the code given in demo. In our Neural Network problem, we are trying to predict the current state using future state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b0b7zrCrH3u"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUGsVg3tvnMG"
      },
      "source": [
        "The maximum goal_steps that we can keep is 200 because that is the contraint of the game. We need to reach the goal by those many epoches. we text this 10000 times that why inital_games is 100000. score_requirement is -198 because we want to keep score towards the right and not the left. as right side has rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spBX6EdernRi"
      },
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "env.reset()\n",
        "goal_steps = 200\n",
        "score_requirement = -198\n",
        "intial_games = 10000"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kMGVsN0pWv2"
      },
      "source": [
        "A particular epoch finishes when top(0.5) position is reached or 200 iterations are finished.  The code was not able to converge to optimal points and the success rate was very poor every time. In order to avoid this, few changes were made to the main program data population. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia9lYXVssVXg"
      },
      "source": [
        "def pre_process_data():\n",
        "    acc_scores = []\n",
        "    train_data = []\n",
        "    for game_index in range(intial_games):\n",
        "        score_game = 0\n",
        "        game_memory = []\n",
        "        prev_obs = []\n",
        "        for step_index in range(goal_steps):\n",
        "            action = random.randrange(0, 3)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(prev_obs) > 0:\n",
        "                game_memory.append([prev_obs, action])\n",
        "                \n",
        "            prev_obs = observation\n",
        "            if observation[0] > -0.2:\n",
        "                reward = 1\n",
        "            \n",
        "            score_game += reward\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "        if score_game >= score_requirement:\n",
        "            acc_scores.append(score_game)\n",
        "            for data in game_memory:\n",
        "                if data[1] == 1:\n",
        "                    output = [0, 1, 0]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1, 0, 0]\n",
        "                elif data[1] == 2:\n",
        "                    output = [0, 0, 1]\n",
        "                train_data.append([data[0], output])\n",
        "        \n",
        "        env.reset()\n",
        "    \n",
        "    print(acc_scores)\n",
        "    \n",
        "    return train_data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDxXEHFp9z2"
      },
      "source": [
        "The rewards given by the environment were taken as 1 when the oberavation was above -0.2. This was done as it could've been the top of the hill which means reward should be given. Now coming to the problem of converging to local minima, after every action, the agent will take negative rewards so that we don't reach the minima. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7DrAsvwsXna",
        "outputId": "a51b67d5-ca3b-49df-d59f-66d7a3bd638e"
      },
      "source": [
        "train_data = pre_process_data()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-176.0, -188.0, -192.0, -186.0, -188.0, -178.0, -192.0, -176.0, -182.0, -190.0, -166.0, -192.0, -174.0, -176.0, -190.0, -192.0, -184.0, -166.0, -190.0, -182.0, -172.0, -190.0, -188.0, -194.0, -170.0, -166.0, -164.0, -168.0, -164.0, -180.0, -164.0, -178.0, -164.0, -192.0, -184.0, -182.0, -194.0, -194.0, -198.0, -186.0, -172.0, -180.0, -178.0, -182.0, -170.0, -182.0, -194.0, -194.0, -198.0, -192.0, -182.0, -176.0, -192.0, -192.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSPub-ybtbbY"
      },
      "source": [
        "def build_model(input_size, output_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=input_size, activation='relu'))\n",
        "        model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(output_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "        return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJlFv-2ZteNo"
      },
      "source": [
        "def train_model(train_data):\n",
        "    X = np.array([i[0] for i in train_data]).reshape(-1, len(train_data[0][0]))\n",
        "    y = np.array([i[1] for i in train_data]).reshape(-1, len(train_data[0][1]))\n",
        "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
        "    \n",
        "    model.fit(X, y, epochs=10)\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74-GnSDtiLf",
        "outputId": "40f7f821-bd1e-4370-f5a0-5a7a022d12be"
      },
      "source": [
        "trained_model = train_model(train_data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2247\n",
            "Epoch 2/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2217\n",
            "Epoch 3/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2203\n",
            "Epoch 4/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2203\n",
            "Epoch 5/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2196\n",
            "Epoch 6/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2193\n",
            "Epoch 7/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2193\n",
            "Epoch 8/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2192\n",
            "Epoch 9/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2193\n",
            "Epoch 10/10\n",
            "336/336 [==============================] - 1s 2ms/step - loss: 0.2192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n6ZrsUttn_f",
        "outputId": "35245692-bb44-4d08-ba6a-787ff70b6238"
      },
      "source": [
        "choices = []\n",
        "scores = []\n",
        "\n",
        "for each_game in range(100):\n",
        "    score = 0\n",
        "    prev_obs = []\n",
        "    for step in range(goal_steps):\n",
        "        if len(prev_obs)==0:\n",
        "            action = random.randrange(0,2)\n",
        "        else:\n",
        "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
        "        \n",
        "        choices.append(action)\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        prev_obs = new_observation\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.reset()\n",
        "    scores.append(score)\n",
        "\n",
        "print(scores)\n",
        "print('Average Score:',sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-101.0, -123.0, -151.0, -122.0, -129.0, -119.0, -119.0, -97.0, -120.0, -119.0, -129.0, -131.0, -91.0, -129.0, -175.0, -129.0, -174.0, -126.0, -127.0, -130.0, -111.0, -129.0, -120.0, -176.0, -147.0, -147.0, -137.0, -124.0, -119.0, -124.0, -97.0, -125.0, -93.0, -121.0, -143.0, -124.0, -122.0, -181.0, -125.0, -130.0, -131.0, -130.0, -118.0, -91.0, -123.0, -149.0, -183.0, -107.0, -131.0, -150.0, -121.0, -131.0, -121.0, -120.0, -119.0, -94.0, -175.0, -120.0, -106.0, -130.0, -131.0, -119.0, -150.0, -135.0, -119.0, -91.0, -129.0, -132.0, -134.0, -119.0, -151.0, -120.0, -130.0, -129.0, -129.0, -101.0, -129.0, -120.0, -95.0, -129.0, -130.0, -129.0, -130.0, -137.0, -174.0, -150.0, -120.0, -137.0, -130.0, -137.0, -183.0, -91.0, -118.0, -131.0, -111.0, -124.0, -128.0, -174.0, -119.0, -130.0]\n",
            "Average Score: -128.61\n",
            "choice 1:0.12028613638130783  choice 0:0.3150610372443822 choice 2:0.5646528263743099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3n5rNRCLvz",
        "outputId": "1d8e24d4-586f-4bbc-c99f-c7e6b1a1a389"
      },
      "source": [
        "print (\"Success rate: \" +  str(abs(sum(scores)/goal_steps)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success rate: 64.305\n"
          ]
        }
      ]
    }
  ]
}