{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 09 Part2.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45BMVQQcNQ6E"
      },
      "source": [
        "# Homework 09 Part 2\n",
        "### Explanations are given below as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo5aOqYaVU8C"
      },
      "source": [
        "##### I did not use the method given in the demo for the hammer problem. Because when I replicated the same method the reward given was always -1. Therefore the value for \"scores\" was always -200. This would result the car in never converging to the top. To combat the problem I had to change the method a little."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b0b7zrCrH3u"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK2rxuXfKRyP"
      },
      "source": [
        "##### The number of goal steps is 200 because the time steps limit in the gym openai source code is 200. Initial games=10000 are the total number of iterations for the game to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spBX6EdernRi"
      },
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "env.reset()\n",
        "goal_steps = 200\n",
        "score_requirement = -198\n",
        "intial_games = 10000"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mX0hoWFYDXd"
      },
      "source": [
        "##### Score, game_memory, previous_observation variables where will store the current gameâ€™s total score and previous step observation and the action taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqEfM6M8YOyP"
      },
      "source": [
        "##### The episode ends when you reach 0.5(top) position, or if 200 iterations are reached. I played several times 10000 times but never reached the top position. So at the time of data population, I changed a small logic that finally gave me the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc4zIg8ZYgQY"
      },
      "source": [
        "Then we will check whether the position of the car which is observation[0] is greater than -0.2 if yes then instead of taking the reward given by our game environment I took as 1 because -0.2 position is top of the hill which means our random actions giving somewhat fruitful results.\n",
        "The agent will earn negative rewards for each action it takes making it very easy to land in the local optima of doing nothing. There are many approcahes implemented, this approach gave me the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia9lYXVssVXg"
      },
      "source": [
        "def model_data_preparation():\n",
        "    training_data = []\n",
        "    accepted_scores = []\n",
        "    for game_index in range(intial_games):\n",
        "        score = 0\n",
        "        game_memory = []\n",
        "        previous_observation = []\n",
        "        for step_index in range(goal_steps):\n",
        "            action = random.randrange(0, 3)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(previous_observation) > 0:\n",
        "                game_memory.append([previous_observation, action])\n",
        "                \n",
        "            previous_observation = observation\n",
        "            if observation[0] > -0.2:\n",
        "                reward = 1\n",
        "            \n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "        if score >= score_requirement:\n",
        "            accepted_scores.append(score)\n",
        "            for data in game_memory:\n",
        "                if data[1] == 1:\n",
        "                    output = [0, 1, 0]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1, 0, 0]\n",
        "                elif data[1] == 2:\n",
        "                    output = [0, 0, 1]\n",
        "                training_data.append([data[0], output])\n",
        "        \n",
        "        env.reset()\n",
        "    \n",
        "    print(accepted_scores)\n",
        "    \n",
        "    return training_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7DrAsvwsXna",
        "outputId": "ec74bef5-bf15-43e5-bc2e-14b7450be4af"
      },
      "source": [
        "training_data = model_data_preparation()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-168.0, -180.0, -194.0, -186.0, -172.0, -192.0, -180.0, -198.0, -168.0, -182.0, -188.0, -188.0, -190.0, -192.0, -186.0, -198.0, -180.0, -174.0, -170.0, -170.0, -182.0, -194.0, -184.0, -194.0, -196.0, -170.0, -178.0, -192.0, -182.0, -174.0, -186.0, -194.0, -176.0, -184.0, -182.0, -184.0, -180.0, -170.0, -172.0, -168.0, -184.0, -182.0, -192.0, -192.0, -182.0, -180.0, -198.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSPub-ybtbbY"
      },
      "source": [
        "def build_model(input_size, output_size):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
        "        model.add(Dense(52, activation='relu'))\n",
        "        model.add(Dense(output_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "        return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJlFv-2ZteNo"
      },
      "source": [
        "def train_model(training_data):\n",
        "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
        "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
        "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
        "    \n",
        "    model.fit(X, y, epochs=5)\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74-GnSDtiLf",
        "outputId": "ceb2277c-0f1b-4225-bb06-196a88320afe"
      },
      "source": [
        "trained_model = train_model(training_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.2294\n",
            "Epoch 2/5\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.2221\n",
            "Epoch 3/5\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.2210\n",
            "Epoch 4/5\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.2204\n",
            "Epoch 5/5\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.2202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n6ZrsUttn_f",
        "outputId": "0705a311-746e-4827-b189-e0c27bce5a73"
      },
      "source": [
        "scores = []\n",
        "choices = []\n",
        "for each_game in range(100):\n",
        "    score = 0\n",
        "    prev_obs = []\n",
        "    for step_index in range(goal_steps):\n",
        "        if len(prev_obs)==0:\n",
        "            action = random.randrange(0,2)\n",
        "        else:\n",
        "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
        "        \n",
        "        choices.append(action)\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        prev_obs = new_observation\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.reset()\n",
        "    scores.append(score)\n",
        "\n",
        "print(scores)\n",
        "print('Average Score:',sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-200.0, -126.0, -130.0, -127.0, -200.0, -128.0, -127.0, -128.0, -127.0, -200.0, -118.0, -117.0, -117.0, -126.0, -122.0, -132.0, -121.0, -200.0, -118.0, -127.0, -127.0, -119.0, -132.0, -117.0, -121.0, -200.0, -126.0, -200.0, -200.0, -200.0, -127.0, -126.0, -126.0, -121.0, -126.0, -120.0, -139.0, -118.0, -126.0, -122.0, -126.0, -119.0, -123.0, -126.0, -128.0, -127.0, -120.0, -200.0, -126.0, -127.0, -123.0, -118.0, -126.0, -125.0, -200.0, -123.0, -127.0, -131.0, -122.0, -127.0, -117.0, -131.0, -200.0, -200.0, -121.0, -120.0, -200.0, -128.0, -127.0, -200.0, -122.0, -125.0, -117.0, -127.0, -200.0, -136.0, -119.0, -119.0, -127.0, -200.0, -129.0, -117.0, -200.0, -119.0, -130.0, -127.0, -200.0, -129.0, -132.0, -123.0, -119.0, -200.0, -132.0, -119.0, -200.0, -126.0, -127.0, -132.0, -118.0, -123.0]\n",
            "Average Score: -139.66\n",
            "choice 1:0.008449090648718316  choice 0:0.21022483173421166 choice 2:0.7813260776170701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqzO4xNTZT2O"
      },
      "source": [
        "##### Our model trains knowing that if it goes to the top, it automatically performs well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3n5rNRCLvz",
        "outputId": "a7b257ba-e937-47a6-bf7d-cfff1b79927b"
      },
      "source": [
        "print (\"Success rate: \" +  str(abs(sum(scores)/goal_steps)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success rate: 69.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKUbQ06C9-1"
      },
      "source": [
        "##### I did not render the video as it was not explicitly mentioned in the homework description. Since colab is not locally hosted, env.render() does not produce any simulations of the gameplay. If you want to render the gameplay please run the below function and it will plot the frames at each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJJUDlO4F9Xo"
      },
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "\n",
        "scores = []\n",
        "choices = []\n",
        "for each_game in range(100):\n",
        "    score = 0\n",
        "    prev_obs = []\n",
        "    for step_index in range(goal_steps):\n",
        "        img.set_data(env.render('rgb_array'))\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        if len(prev_obs)==0:\n",
        "            action = random.randrange(0,2)\n",
        "        else:\n",
        "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
        "        \n",
        "        choices.append(action)\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        prev_obs = new_observation\n",
        "        score+=reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.reset()\n",
        "    scores.append(score)\n",
        "\n",
        "print(scores)\n",
        "print('Average Score:',sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOniUHcXHsGL"
      },
      "source": [
        "##### I ran the above simulation as well and my car moved up the mountain and successfully converged to the optimal solution rather than converging to the local minima."
      ]
    }
  ]
}