{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW_8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baOV3Y0E4K2V"
      },
      "source": [
        "## Step1. Install AI Gym, the instructions can be found at https://gym.openai.com/docs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4rIRcfeUYfU"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNA4Y8aa4O-8"
      },
      "source": [
        "## Step2. Import the FrozenLake-v0 environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ziw2CjOUYc8"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARm7lXUX_-Gd"
      },
      "source": [
        "Rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4yihWxSUYYN"
      },
      "source": [
        "n_size = env.action_space.n\n",
        "m_size = env.observation_space.n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezixiRN-UYVo",
        "outputId": "2a17e39d-212e-4241-e4af-b0a210202f54"
      },
      "source": [
        "qtable = np.zeros((m_size, n_size))\n",
        "print(qtable)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFLkJuxUAI9E"
      },
      "source": [
        "### Specifying Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTm5kKggUYRL"
      },
      "source": [
        "total_episodes = 15000\n",
        "learning_rate = 0.1\n",
        "max_steps = 120\n",
        "gamma = 0.95\n",
        "epsilon = 0.9\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.01 \n",
        "decay_rate = 0.001"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdfEpWfr9x3r"
      },
      "source": [
        "## Step3. Train a model using Q-learning and generate a Q-table, save this table as separate file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efGeIYPbUYOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ac5d49-bdf9-40ad-8bad-6f3a682cd8df"
      },
      "source": [
        "finalRewards = []\n",
        "\n",
        "start_time=time.time()\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        ## First we randomize a number\n",
        "        exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "    \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]     \n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # Dead state\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Epsilon reduction as we want to limit scope\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    finalRewards.append(total_rewards)\n",
        "    \n",
        "end_time=time.time()\n",
        "\n",
        "print (\"Success rate: \" +  str(sum(finalRewards)/total_episodes))\n",
        "print(\"Time taken to train the model:\", round((end_time-start_time)/60,2),\"mins\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success rate: 0.5396\n",
            "Time taken to train the model: 0.43 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XGCa6U46tnB",
        "outputId": "c9411f74-996e-4a7c-e7d6-a61e26462f7b"
      },
      "source": [
        "df=pd.DataFrame(qtable)\n",
        "df.to_csv('qtable.csv')\n",
        "print(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0         1         2         3\n",
            "0   0.185419  0.153966  0.157922  0.154258\n",
            "1   0.094987  0.063773  0.084645  0.150995\n",
            "2   0.156970  0.100657  0.090816  0.092033\n",
            "3   0.025658  0.060784  0.009628  0.009657\n",
            "4   0.214032  0.158227  0.146966  0.151399\n",
            "5   0.000000  0.000000  0.000000  0.000000\n",
            "6   0.077205  0.078083  0.199557  0.060935\n",
            "7   0.000000  0.000000  0.000000  0.000000\n",
            "8   0.141160  0.171727  0.179706  0.289443\n",
            "9   0.260007  0.394554  0.266592  0.238510\n",
            "10  0.367916  0.259917  0.147884  0.203182\n",
            "11  0.000000  0.000000  0.000000  0.000000\n",
            "12  0.000000  0.000000  0.000000  0.000000\n",
            "13  0.282553  0.379642  0.460882  0.378357\n",
            "14  0.488717  0.683225  0.574123  0.517127\n",
            "15  0.000000  0.000000  0.000000  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3TkML1E95pu"
      },
      "source": [
        "## Step4. Discuss the parameters used to produce you training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN_bcKlS98eT"
      },
      "source": [
        "#total_episodes = 15000\n",
        "#learning_rate = 0.1\n",
        "#max_steps = 120\n",
        "#gamma = 0.95\n",
        "#epsilon = 0.9\n",
        "#max_epsilon = 0.9\n",
        "#min_epsilon = 0.01 \n",
        "#decay_rate = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CEQZMjsBb6T"
      },
      "source": [
        "1) The Agent tries out various'state-action' combinations until it either achieves its goal or falls into the pit. Each of these investigations will be referred to as an episode. We begin the following episode each time the agent reaches the target or is terminated.</br>\n",
        "2) The learning rate tells us how fast we want to reach the global minima. Specifying a huge learning rate could result in not reaching global minima. </br>\n",
        "3) max_steps is the max steps that would be required to reach the goal. The max step should be kept a little high so that we reach the goal in every iteration. </br>\n",
        "4)  We want the agent to execute random acts when it first starts learning so that it can explore more paths. The Q-function converges to increasingly consistent Q-values as the agent improves. Now we want our agent to conduct greedy actions and exploit pathways with the highest Q-value. This is where epsilon enters the picture.\n",
        "For probability (1-epsilon), the agent takes random actions, while for probability (1-epsilon), it takes greedy actions.</br>\n",
        "5) A decaying -greedy action selection was employed by Google DeepMind. Where decays from 1 to 0.1 over time â€” at first, the system makes fully random motions to maximize its exploration of the state space, and eventually it settles down to a fixed exploration rate. That if why I have used max_epsilon=0.9 and min_epsilon=0.01 with a decay rate of 0.001 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQrc6sjmETbj"
      },
      "source": [
        "## Step5. Discuss the results in terms of success rate, the time it took to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTZYuDk8DyVd",
        "outputId": "fa67fdfc-fae4-4e57-bbcc-326f06f1ce40"
      },
      "source": [
        "print (\"Success Rate: \" +  str(sum(finalRewards)/total_episodes))\n",
        "print(\"Time taken to train the model:\", round((end_time-start_time)/60,2),\"min\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Rate: 0.5396\n",
            "Time taken to train the model: 0.43 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbVWSPBVEg-H"
      },
      "source": [
        "We can see that out of 15000 iterations, we have reached a goal 53.96% of the times which is our success rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8l2iIhIOvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}