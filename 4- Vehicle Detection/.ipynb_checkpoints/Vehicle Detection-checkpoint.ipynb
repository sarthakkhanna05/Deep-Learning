{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model and it's architecture \n",
    "prototxt_file = \"deploy.prototxt\"\n",
    "model_file = \"mobilenet_iter_73000.caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes MobileNet can detect \n",
    "classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "    \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "    \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "    \"sofa\", \"train\", \"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Collect a source video. It may be necessary to divide the video into discrete image frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Conduct inference on each frame of the video, drawing bounding boxes around detected vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv.dnn.readNetFromCaffe(prototxt_file, model_file)\n",
    "\n",
    "def frame_processing(next_f):\n",
    "    rgb = cv.cvtColor(next_f, cv.COLOR_BGR2RGB)\n",
    "    (height, width) = next_f.shape[:2]\n",
    "\n",
    "    # Creating a blob of frame so that it can pass through the model \n",
    "    \n",
    "    blob = cv.dnn.blobFromImage(next_f, size=(300, 300), ddepth=cv.CV_8U)\n",
    "    net.setInput(blob, scalefactor=1.0/127.5, mean=[127.5, 127.5, 127.5])\n",
    "    prediction = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, prediction.shape[2]):\n",
    "        # Confidence for the prediction\n",
    "        \n",
    "        confidence = prediction[0, 0, i, 2]\n",
    "        # Filter those predictions which aren't close to the confidence of this\n",
    "        \n",
    "        if confidence > 0.4:\n",
    "            # getting the idx of prediction\n",
    "            \n",
    "            idx = int(prediction[0, 0, i, 1])\n",
    "            \n",
    "            if classes[idx] != \"car\":\n",
    "                continue\n",
    "            \n",
    "            box = prediction[0, 0, i, 3:7] * np.array([width, height, width, height ])\n",
    "            (s_X, s_Y, e_X, e_Y) = box.astype(\"int\")\n",
    "            \n",
    "            cv.rectangle(next_f, (s_X, s_Y), (e_X, e_Y), (0, 255, 0), 3)\n",
    "\n",
    "    return next_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Format the results back into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardetection(filename):\n",
    "    cap = cv.VideoCapture(filename)\n",
    "\n",
    "    # output file dimensions\n",
    "    width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Output file type \n",
    "    fps = 20\n",
    "    size = (int(width),int(height))\n",
    "    \n",
    "    # step 3: formatting into video \n",
    "    format_v = cv.VideoWriter_fourcc('m','p','4','v')\n",
    "    output = cv.VideoWriter()\n",
    "    success = output.open('output_video.mp4', format_v, fps, size, True)\n",
    "\n",
    "    print(\"Algorithm Successfully started\")\n",
    "    \n",
    "    ### Step 1. Collect a source video. It may be necessary to divide the video into discrete image frames\n",
    "    while True:\n",
    "        ret, next_f = cap.read() # To read next video frame into memory \n",
    "        \n",
    "        if ret == False: break\n",
    "\n",
    "        next_f = frame_processing(next_f)\n",
    "        \n",
    "        # step 3: formatting into video \n",
    "        output.write(next_f)\n",
    "        \n",
    "        key = cv.waitKey(50)\n",
    "        \n",
    "        if key == 27: # Hit ESC key to stop\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"Car detection video successfully generated\")\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    output.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Successfully started\n",
      "Car detection video successfully generated\n"
     ]
    }
   ],
   "source": [
    "### Step 1. Collect a source video. It may be necessary to divide the video into discrete image frames\n",
    "## paste the link to the video here \n",
    "cardetection('raw_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and Observations:\n",
    "\n",
    "In this Algorithm I've use the OpenCV libraries to split the video into frames. The cv.read() reads frame into the memory and that frame is processed using frame_processing function. after processing it, the frame is appeneded into an outpus file. Pre-trained MobileNet model is used to detect images. There are a lot of classes that the MobileNet dataset can detect but here I've used it to just detect Cars. Opencv let's you use a pre-trained model and using cafee, we can pass the drame through a deep CNN model. \n",
    "\n",
    "The prototext file has the structure of the Neural Netwoek and .caffemodel file has a pre-trained mobilenet model built by caffe. \n",
    "\n",
    "The reason that the video is not able to detect far off cars is the confidence level. When the algorithm is really sure it's a car, then only it can detect it as a car. That only happens when the car is closer in the frame and fetches features that result in high confidence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
